{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Text2Image FFT.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afiaka87/deep-daze-ffm/blob/main/Text2Image_FFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toWe1IoH7X35"
      },
      "source": [
        "# Text to Image\r\n",
        "\r\n",
        "Based on [CLIP](https://github.com/openai/CLIP) + FFT from [Lucent](https://github.com/greentfrapp/lucent) // made by [eps696](https://github.com/eps696) [Vadim Epstein]  \r\n",
        "thanks to [Ryan Murdock](https://rynmurdock.github.io/), [Jonathan Fly](https://twitter.com/jonathanfly), [@tg-bomze](https://github.com/tg-bomze) \r\n",
        "\r\n",
        "## Features \r\n",
        "* complex requests:\r\n",
        "  * image and/or text as main prompts  \r\n",
        "   (composition similarity controlled with [SSIM](https://github.com/Po-Hsun-Su/pytorch-ssim) loss)\r\n",
        "  * additional text prompts for fine details and to subtract (avoid) things\r\n",
        "  * criteria inversion (show \"the opposite\")\r\n",
        "\r\n",
        "* generates [FFT-encoded](https://github.com/greentfrapp/lucent/blob/master/lucent/optvis/param/spatial.py) image (massive detailed textures, a la deepdream)\r\n",
        "* ! fast convergence\r\n",
        "* ! undemanding for RAM - fullHD/4K and above\r\n",
        "* can use both CLIP models at once (ViT and RN50)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QytcEMSKBtN-"
      },
      "source": [
        "**Run this cell after each session restart**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etzxXVZ_r-Nf",
        "cellView": "form"
      },
      "source": [
        "#@title General setup\r\n",
        "\r\n",
        "import subprocess\r\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\r\n",
        "print(\"CUDA version:\", CUDA_version)\r\n",
        "\r\n",
        "if CUDA_version == \"10.0\":\r\n",
        "    torch_version_suffix = \"+cu100\"\r\n",
        "elif CUDA_version == \"10.1\":\r\n",
        "    torch_version_suffix = \"+cu101\"\r\n",
        "elif CUDA_version == \"10.2\":\r\n",
        "    torch_version_suffix = \"\"\r\n",
        "else:\r\n",
        "    torch_version_suffix = \"+cu110\"\r\n",
        "\r\n",
        "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\r\n",
        "\r\n",
        "try: \r\n",
        "  !pip3 install googletrans==3.1.0a0\r\n",
        "  from googletrans import Translator, constants\r\n",
        "  # from pprint import pprint\r\n",
        "  translator = Translator()\r\n",
        "except: pass\r\n",
        "!pip install ftfy\r\n",
        "\r\n",
        "import os\r\n",
        "import time\r\n",
        "from math import exp\r\n",
        "import random\r\n",
        "import imageio\r\n",
        "import numpy as np\r\n",
        "import PIL\r\n",
        "from skimage import exposure\r\n",
        "from base64 import b64encode\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision\r\n",
        "from torch.autograd import Variable\r\n",
        "\r\n",
        "from IPython.display import HTML, Image, display, clear_output\r\n",
        "from IPython.core.interactiveshell import InteractiveShell\r\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\r\n",
        "import ipywidgets as ipy\r\n",
        "# import glob\r\n",
        "from google.colab import output, files\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "\r\n",
        "!git clone https://github.com/openai/CLIP.git\r\n",
        "%cd /content/CLIP/\r\n",
        "import clip\r\n",
        "perceptor, preprocess = clip.load('ViT-B/32')\r\n",
        "model_vit, _ = clip.load('ViT-B/32')\r\n",
        "\r\n",
        "workdir = '_out'\r\n",
        "tempdir = os.path.join(workdir, 'ttt')\r\n",
        "os.makedirs(tempdir, exist_ok=True)\r\n",
        "\r\n",
        "clear_output()\r\n",
        "\r\n",
        "###  FFT from Lucent library  https://github.com/greentfrapp/lucent\r\n",
        "\r\n",
        "def pixel_image(shape, sd=2.):\r\n",
        "    tensor = (torch.randn(*shape) * sd).cuda().requires_grad_(True)\r\n",
        "    return [tensor], lambda: tensor\r\n",
        "\r\n",
        "# From https://github.com/tensorflow/lucid/blob/master/lucid/optvis/param/spatial.py\r\n",
        "def rfft2d_freqs(h, w):\r\n",
        "    \"\"\"Computes 2D spectrum frequencies.\"\"\"\r\n",
        "    fy = np.fft.fftfreq(h)[:, None]\r\n",
        "    # when we have an odd input dimension we need to keep one additional frequency and later cut off 1 pixel\r\n",
        "    if w % 2 == 1:\r\n",
        "        fx = np.fft.fftfreq(w)[: w // 2 + 2]\r\n",
        "    else:\r\n",
        "        fx = np.fft.fftfreq(w)[: w // 2 + 1]\r\n",
        "    return np.sqrt(fx * fx + fy * fy)\r\n",
        "\r\n",
        "def fft_image(shape, sd=0.1, decay_power=1., smooth_col=1.):\r\n",
        "    batch, channels, h, w = shape\r\n",
        "    freqs = rfft2d_freqs(h, w)\r\n",
        "    init_val_size = (batch, channels) + freqs.shape + (2,) # 2 for imaginary and real components\r\n",
        "    spectrum_real_imag_t = (torch.randn(*init_val_size) * sd).cuda().requires_grad_(True)\r\n",
        "    scale = 1.0 / np.maximum(freqs, 1.0 / max(w, h)) ** decay_power\r\n",
        "    scale = torch.tensor(scale).float()[None, None, ..., None].cuda()\r\n",
        "\r\n",
        "    def inner():\r\n",
        "        scaled_spectrum_t = scale * spectrum_real_imag_t\r\n",
        "        image = torch.irfft(scaled_spectrum_t, 2, normalized=True, signal_sizes=(h, w))\r\n",
        "        image = image[:batch, :channels, :h, :w]\r\n",
        "        image = image / (1 + image.std())**1.3 # keep contrast\r\n",
        "        image = image * 4. / smooth_col # more desaturation, smoothen colors & contrast\r\n",
        "        return image\r\n",
        "    return [spectrum_real_imag_t], inner\r\n",
        "\r\n",
        "def to_valid_rgb(image_f, decorrelate=True):\r\n",
        "    def inner():\r\n",
        "        image = image_f()\r\n",
        "        if decorrelate:\r\n",
        "            image = _linear_decorrelate_color(image)\r\n",
        "        return torch.sigmoid(image)\r\n",
        "    return inner\r\n",
        "    \r\n",
        "def _linear_decorrelate_color(tensor):\r\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "    t_permute = tensor.permute(0,2,3,1)\r\n",
        "    t_permute = torch.matmul(t_permute, torch.tensor(color_correlation_normalized.T).to(device))\r\n",
        "    tensor = t_permute.permute(0,3,1,2)\r\n",
        "    return tensor\r\n",
        "\r\n",
        "color_correlation_svd_sqrt = np.asarray([[0.26, 0.09, 0.02],\r\n",
        "                                         [0.27, 0.00, -0.05],\r\n",
        "                                         [0.27, -0.09, 0.03]]).astype(\"float32\")\r\n",
        "max_norm_svd_sqrt = np.max(np.linalg.norm(color_correlation_svd_sqrt, axis=0))\r\n",
        "color_correlation_normalized = color_correlation_svd_sqrt / max_norm_svd_sqrt\r\n",
        "\r\n",
        "### Libs\r\n",
        "\r\n",
        "def slice_imgs(imgs, count, transform=None, uniform=False, micro=None):\r\n",
        "  def map(x, a, b):\r\n",
        "    return x * (b-a) + a\r\n",
        "\r\n",
        "  rnd_size = torch.rand(count)\r\n",
        "  if uniform is True:\r\n",
        "    rnd_offx = torch.rand(count)\r\n",
        "    rnd_offy = torch.rand(count)\r\n",
        "  else: # normal around center\r\n",
        "    rnd_offx = torch.clip(torch.randn(count) * 0.2 + 0.5, 0, 1) \r\n",
        "    rnd_offy = torch.clip(torch.randn(count) * 0.2 + 0.5, 0, 1)\r\n",
        "  \r\n",
        "  sz = [img.shape[2:] for img in imgs]\r\n",
        "  sz_min = [np.min(s) for s in sz]\r\n",
        "  if uniform is True:\r\n",
        "    sz = [[2*s[0], 2*s[1]] for s in list(sz)]\r\n",
        "    imgs = [pad_up_to(imgs[i], sz[i], type='centr') for i in range(len(imgs))]\r\n",
        "\r\n",
        "  sliced = []\r\n",
        "  for i, img in enumerate(imgs):\r\n",
        "    cuts = []\r\n",
        "    for c in range(count):\r\n",
        "      if micro is True: # both scales, micro mode\r\n",
        "        csize = map(rnd_size[c], 64, max(224, 0.25*sz_min[i])).int()\r\n",
        "      elif micro is False: # both scales, macro mode\r\n",
        "        csize = map(rnd_size[c], 0.5*sz_min[i], 0.98*sz_min[i]).int()\r\n",
        "      else: # single scale\r\n",
        "        csize = map(rnd_size[c], 64, 0.98*sz_min[i]).int()\r\n",
        "      offsetx = map(rnd_offx[c], 0, sz[i][1] - csize).int()\r\n",
        "      offsety = map(rnd_offy[c], 0, sz[i][0] - csize).int()\r\n",
        "      cut = img[:, :, offsety:offsety + csize, offsetx:offsetx + csize]\r\n",
        "      cut = torch.nn.functional.interpolate(cut, (224,224), mode='bicubic')\r\n",
        "      if transform is not None: \r\n",
        "        cut = transform(cut)\r\n",
        "      cuts.append(cut)\r\n",
        "    sliced.append(torch.cat(cuts, 0))\r\n",
        "  return sliced\r\n",
        "\r\n",
        "def makevid(seq_dir, size=None):\r\n",
        "  out_sequence = seq_dir + '/%03d.jpg'\r\n",
        "  out_video = seq_dir + '.mp4'\r\n",
        "  !ffmpeg -y -v warning -i $out_sequence $out_video\r\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(open(out_video,'rb').read()).decode()\r\n",
        "  wh = '' if size is None else 'width=%d height=%d' % (size, size)\r\n",
        "  return \"\"\"<video %s controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % (wh, data_url)\r\n",
        "\r\n",
        "# Tiles an array around two points, allowing for pad lengths greater than the input length\r\n",
        "# adapted from https://discuss.pytorch.org/t/symmetric-padding/19866/3\r\n",
        "def tile_pad(xt, padding):\r\n",
        "  h, w = xt.shape[-2:]\r\n",
        "  left, right, top, bottom = padding\r\n",
        "\r\n",
        "  def tile(x, minx, maxx):\r\n",
        "    rng = maxx - minx\r\n",
        "    mod = np.remainder(x - minx, rng)\r\n",
        "    out = mod + minx\r\n",
        "    return np.array(out, dtype=x.dtype)\r\n",
        "\r\n",
        "  x_idx = np.arange(-left, w+right)\r\n",
        "  y_idx = np.arange(-top, h+bottom)\r\n",
        "  x_pad = tile(x_idx, -0.5, w-0.5)\r\n",
        "  y_pad = tile(y_idx, -0.5, h-0.5)\r\n",
        "  xx, yy = np.meshgrid(x_pad, y_pad)\r\n",
        "  return xt[..., yy, xx]\r\n",
        "\r\n",
        "def pad_up_to(x, size, type='centr'):\r\n",
        "  sh = x.shape[2:][::-1]\r\n",
        "  if list(x.shape[2:]) == list(size): return x\r\n",
        "  padding = []\r\n",
        "  for i, s in enumerate(size[::-1]):\r\n",
        "    if 'side' in type.lower():\r\n",
        "      padding = padding + [0, s-sh[i]]\r\n",
        "    else: # centr\r\n",
        "      p0 = (s-sh[i]) // 2\r\n",
        "      p1 = s-sh[i] - p0\r\n",
        "      padding = padding + [p0,p1]\r\n",
        "  y = tile_pad(x, padding)\r\n",
        "  return y\r\n",
        "\r\n",
        "class ProgressBar(object):\r\n",
        "  def __init__(self, task_num=10):\r\n",
        "    self.pbar = ipy.IntProgress(min=0, max=task_num, bar_style='') # (value=0, min=0, max=max, step=1, description=description, bar_style='')\r\n",
        "    self.labl = ipy.Label()\r\n",
        "    display(ipy.HBox([self.pbar, self.labl]))\r\n",
        "    self.task_num = task_num\r\n",
        "    self.completed = 0\r\n",
        "    self.start()\r\n",
        "\r\n",
        "  def start(self, task_num=None):\r\n",
        "    if task_num is not None:\r\n",
        "      self.task_num = task_num\r\n",
        "    if self.task_num > 0:\r\n",
        "      self.labl.value = '0/{}'.format(self.task_num)\r\n",
        "    else:\r\n",
        "      self.labl.value = 'completed: 0, elapsed: 0s'\r\n",
        "    self.start_time = time.time()\r\n",
        "\r\n",
        "  def upd(self, *p, **kw):\r\n",
        "    self.completed += 1\r\n",
        "    elapsed = time.time() - self.start_time + 0.0000000000001\r\n",
        "    fps = self.completed / elapsed if elapsed>0 else 0\r\n",
        "    if self.task_num > 0:\r\n",
        "      finaltime = time.asctime(time.localtime(self.start_time + self.task_num * elapsed / float(self.completed)))\r\n",
        "      fin = ' end %s' % finaltime[11:16]\r\n",
        "      percentage = self.completed / float(self.task_num)\r\n",
        "      eta = int(elapsed * (1 - percentage) / percentage + 0.5)\r\n",
        "      self.labl.value = '{}/{}, rate {:.3g}s, time {}s, left {}s, {}'.format(self.completed, self.task_num, 1./fps, shortime(elapsed), shortime(eta), fin)\r\n",
        "    else:\r\n",
        "      self.labl.value = 'completed {}, time {}s, {:.1f} steps/s'.format(self.completed, int(elapsed + 0.5), fps)\r\n",
        "    self.pbar.value += 1\r\n",
        "    if self.completed == self.task_num: self.pbar.bar_style = 'success'\r\n",
        "    return \r\n",
        "    # return self.completed\r\n",
        "\r\n",
        "def time_days(sec):\r\n",
        "  return '%dd %d:%02d:%02d' % (sec/86400, (sec/3600)%24, (sec/60)%60, sec%60)\r\n",
        "def time_hrs(sec):\r\n",
        "  return '%d:%02d:%02d' % (sec/3600, (sec/60)%60, sec%60)\r\n",
        "def shortime(sec):\r\n",
        "  if sec < 60:\r\n",
        "    time_short = '%d' % (sec)\r\n",
        "  elif sec < 3600:\r\n",
        "    time_short  = '%d:%02d' % ((sec/60)%60, sec%60)\r\n",
        "  elif sec < 86400:\r\n",
        "    time_short  = time_hrs(sec)\r\n",
        "  else:\r\n",
        "    time_short = time_days(sec)\r\n",
        "  return time_short\r\n",
        "\r\n",
        "# from https://github.com/Po-Hsun-Su/pytorch-ssim\r\n",
        "\r\n",
        "def gaussian(window_size, sigma):\r\n",
        "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\r\n",
        "    return gauss/gauss.sum()\r\n",
        "\r\n",
        "def create_window(window_size, channel):\r\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\r\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\r\n",
        "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\r\n",
        "    return window\r\n",
        "\r\n",
        "def _ssim(img1, img2, window, window_size, channel, size_average = True):\r\n",
        "  mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\r\n",
        "  mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\r\n",
        "  mu1_sq = mu1.pow(2)\r\n",
        "  mu2_sq = mu2.pow(2)\r\n",
        "  mu1_mu2 = mu1*mu2\r\n",
        "  sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\r\n",
        "  sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\r\n",
        "  sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\r\n",
        "  C1 = 0.01**2\r\n",
        "  C2 = 0.03**2\r\n",
        "  ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\r\n",
        "  if size_average:\r\n",
        "    return ssim_map.mean()\r\n",
        "  else:\r\n",
        "    return ssim_map.mean(1).mean(1).mean(1)\r\n",
        "\r\n",
        "class SSIM(torch.nn.Module):\r\n",
        "  def __init__(self, window_size = 11, size_average = True):\r\n",
        "    super(SSIM, self).__init__()\r\n",
        "    self.window_size = window_size\r\n",
        "    self.size_average = size_average\r\n",
        "    self.channel = 1\r\n",
        "    self.window = create_window(window_size, self.channel)\r\n",
        "\r\n",
        "  def forward(self, img1, img2):\r\n",
        "    (_, channel, _, _) = img1.size()\r\n",
        "    if channel == self.channel and self.window.data.type() == img1.data.type():\r\n",
        "      window = self.window\r\n",
        "    else:\r\n",
        "      window = create_window(self.window_size, channel)\r\n",
        "      if img1.is_cuda:\r\n",
        "        window = window.cuda(img1.get_device())\r\n",
        "      window = window.type_as(img1)\r\n",
        "      self.window = window\r\n",
        "      self.channel = channel\r\n",
        "    return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\r\n",
        "\r\n",
        "!nvidia-smi -L\r\n",
        "print('\\nDone!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbJ9K4Cq8MtB"
      },
      "source": [
        "Type some `text` and/or upload some image to start.  \r\n",
        "`fine_details` input would add micro details of that topic.  \r\n",
        "Put to `subtract` the topics, which you would like to avoid in the result.  \r\n",
        "*NB: more prompts = more memory! (handled by auto-decreasing `samples` amount, hopefully you don't need to act).*  \r\n",
        "`invert` the whole criteria, if you want to see \"the totally opposite\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUvpdy8BWGuM",
        "cellView": "form"
      },
      "source": [
        "#@title Input\r\n",
        "\r\n",
        "text = \"a recursive forest\" #@param {type:\"string\"}\r\n",
        "fine_details = \"the texture of a dead tree\" #@param {type:\"string\"}\r\n",
        "subtract = \"sharpness\" #@param {type:\"string\"}\r\n",
        "translate = False #@param {type:\"boolean\"}\r\n",
        "invert = False #@param {type:\"boolean\"}\r\n",
        "upload_image = True #@param {type:\"boolean\"}\r\n",
        "\r\n",
        "if translate:\r\n",
        "  text = translator.translate(text, dest='en').text\r\n",
        "if upload_image:\r\n",
        "  uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Sj0fxmtw6K"
      },
      "source": [
        "`uniform` option produces seamlessly tileable texture (when off, it's centered).  \r\n",
        "`sync` value adds SSIM loss between the output and input image (if there's one), allowing to \"redraw\" it with controlled similarity.  \r\n",
        "`smooth_col` scaler desaturates image. *There's some empirical auto-tuning in place already, so hopefully it's not really needed anymore.*  \r\n",
        "\r\n",
        "Turn on `dual_model` to optimize with both CLIP models at once (eats more RAM!).  \r\n",
        "Decrease `samples` if you face OOM for higher resolutions (especially when several prompts are used with dual model).  \r\n",
        "Setting `steps` much higher (1000-..) will elaborate details much better, but will start throwing texts like graffiti everywhere."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq0wA-wc-P-s",
        "cellView": "form"
      },
      "source": [
        "#@title Generate\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/GDrive')\n",
        "# clipsDir = '/content/GDrive/MyDrive/T2I ' + dtNow.strftime(\"%Y-%m-%d %H%M\")\n",
        "\n",
        "!rm -rf tempdir\n",
        "\n",
        "sideX =  1920#@param {type:\"integer\"}\n",
        "sideY =  1080#@param {type:\"integer\"}\n",
        "#@markdown > Tweaks & tuning\n",
        "dual_model = False #@param {type:\"boolean\"}\n",
        "uniform = False #@param {type:\"boolean\"}\n",
        "sync =  0 #@param {type:\"number\"}\n",
        "smooth_col =  1.#@param {type:\"number\"}\n",
        "#@markdown > Training\n",
        "steps = 1000 #@param {type:\"integer\"}\n",
        "samples = 128 #@param {type:\"integer\"}\n",
        "learning_rate = .05 #@param {type:\"number\"}\n",
        "#@markdown > Misc\n",
        "save_freq =  50#@param {type:\"integer\"}\n",
        "audio_notification = False #@param {type:\"boolean\"}\n",
        "\n",
        "if dual_model is True:\n",
        "  print(' using dual-model optimization')\n",
        "  model_rn, _ = clip.load('RN50')\n",
        "  samples = samples // 2\n",
        "if len(fine_details) > 0:\n",
        "  samples = int(samples * 0.9)\n",
        "if len(subtract) > 0:\n",
        "  samples = int(samples * 0.9)\n",
        "print(' using %d samples' % samples)\n",
        "\n",
        "norm_in = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "sign = 1. if invert is True else -1.\n",
        "\n",
        "if upload_image:\n",
        "  in_img = list(uploaded.values())[0]\n",
        "  print(' image:', list(uploaded)[0])\n",
        "  img_in = torch.from_numpy(imageio.imread(in_img).astype(np.float32)/255.).unsqueeze(0).permute(0,3,1,2).cuda()\n",
        "  in_sliced = slice_imgs([img_in], samples, transform=norm_in)[0]\n",
        "  img_enc = model_vit.encode_image(in_sliced).detach().clone()\n",
        "  if dual_model is True:\n",
        "    img_enc = torch.cat((img_enc, model_rn.encode_image(in_sliced).detach().clone()), 1)\n",
        "  if sync > 0:\n",
        "    ssim_loss = SSIM(window_size = 11)\n",
        "    img_in = F.interpolate(img_in, (sideY, sideX)).float()\n",
        "  else:\n",
        "    del img_in\n",
        "  del in_sliced; torch.cuda.empty_cache()\n",
        "\n",
        "if len(text) > 2:\n",
        "  print(' macro:', text)\n",
        "  if translate:\n",
        "    translator = Translator()\n",
        "    text = translator.translate(text, dest='en').text\n",
        "    print(' translated to:', text) \n",
        "  tx = clip.tokenize(text)\n",
        "  txt_enc = model_vit.encode_text(tx.cuda()).detach().clone()\n",
        "  if dual_model is True:\n",
        "    txt_enc = torch.cat((txt_enc, model_rn.encode_text(tx.cuda()).detach().clone()), 1)\n",
        "\n",
        "if len(fine_details) > 0:\n",
        "  print(' micro:', fine_details)\n",
        "  if translate:\n",
        "      translator = Translator()\n",
        "      fine_details = translator.translate(fine_details, dest='en').text\n",
        "      print(' translated to:', fine_details) \n",
        "  tx2 = clip.tokenize(fine_details)\n",
        "  txt_enc2 = model_vit.encode_text(tx2.cuda()).detach().clone()\n",
        "  if dual_model is True:\n",
        "      txt_enc2 = torch.cat((txt_enc2, model_rn.encode_text(tx2.cuda()).detach().clone()), 1)\n",
        "\n",
        "if len(subtract) > 0:\n",
        "  print(' without:', subtract)\n",
        "  if translate:\n",
        "      translator = Translator()\n",
        "      subtract = translator.translate(subtract, dest='en').text\n",
        "      print(' translated to:', subtract) \n",
        "  tx0 = clip.tokenize(subtract)\n",
        "  txt_enc0 = model_vit.encode_text(tx0.cuda()).detach().clone()\n",
        "  if dual_model is True:\n",
        "      txt_enc0 = torch.cat((txt_enc0, model_rn.encode_text(tx0.cuda()).detach().clone()), 1)\n",
        "\n",
        "shape = [1, 3, sideY, sideX]\n",
        "param_f = fft_image \n",
        "# param_f = pixel_image\n",
        "# learning_rate = 1.\n",
        "params, image_f = param_f(shape, smooth_col=smooth_col)\n",
        "image_f = to_valid_rgb(image_f)\n",
        "optimizer = torch.optim.Adam(params, learning_rate)\n",
        "\n",
        "def displ(img, fname=None):\n",
        "  img = np.array(img)[:,:,:]\n",
        "  img = np.transpose(img, (1,2,0))  \n",
        "  img = exposure.equalize_adapthist(np.clip(img, 0., 1.))\n",
        "  img = np.clip(img*255, 0, 255).astype(np.uint8)\n",
        "  if fname is not None:\n",
        "    imageio.imsave(fname, np.array(img))\n",
        "    imageio.imsave('result.jpg', np.array(img))\n",
        "\n",
        "def checkin(num):\n",
        "  with torch.no_grad():\n",
        "    img = image_f().cpu().numpy()[0]\n",
        "  displ(img, os.path.join(tempdir, '%03d.jpg' % num))\n",
        "  outpic.clear_output()\n",
        "  with outpic:\n",
        "    display(Image('result.jpg'))\n",
        "\n",
        "def train(i):\n",
        "  loss = 0\n",
        "  img_out = image_f()\n",
        "\n",
        "  micro = False if len(fine_details) > 0 else None\n",
        "  imgs_sliced = slice_imgs([img_out], samples, norm_in, uniform=uniform, micro=micro)\n",
        "  out_enc = model_vit.encode_image(imgs_sliced[-1])\n",
        "  if dual_model is True: # use both clip models\n",
        "      out_enc = torch.cat((out_enc, model_rn.encode_image(imgs_sliced[-1])), 1)\n",
        "  if upload_image:\n",
        "      loss += sign * 100*torch.cosine_similarity(img_enc, out_enc, dim=-1).mean()\n",
        "  if len(text) > 0: # input text\n",
        "      loss += sign * 100*torch.cosine_similarity(txt_enc, out_enc, dim=-1).mean()\n",
        "  if len(subtract) > 0: # subtract text\n",
        "      loss += -sign * 100*torch.cosine_similarity(txt_enc0, out_enc, dim=-1).mean()\n",
        "  if sync > 0 and upload_image: # image composition sync\n",
        "      loss *= 1. + sync * (steps/(i+1) * ssim_loss(img_out, img_in) - 1)\n",
        "  if len(fine_details) > 0: # input text for micro details\n",
        "      imgs_sliced = slice_imgs([img_out], samples, norm_in, uniform=uniform, micro=True)\n",
        "      out_enc2 = model_vit.encode_image(imgs_sliced[-1])\n",
        "      if dual_model is True:\n",
        "          out_enc2 = torch.cat((out_enc2, model_rn.encode_image(imgs_sliced[-1])), 1)\n",
        "      loss += sign * 100*torch.cosine_similarity(txt_enc2, out_enc2, dim=-1).mean()\n",
        "      del out_enc2; torch.cuda.empty_cache()\n",
        "  del img_out, imgs_sliced, out_enc; torch.cuda.empty_cache()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "  if i % save_freq == 0:\n",
        "    checkin(i // save_freq)\n",
        "\n",
        "outpic = ipy.Output()\n",
        "outpic\n",
        "\n",
        "pbar = ProgressBar(steps)\n",
        "for i in range(steps):\n",
        "  train(i)\n",
        "  _ = pbar.upd()\n",
        "\n",
        "HTML(makevid(tempdir))\n",
        "files.download('_out/ttt.mp4')\n",
        "if audio_notification == True: output.eval_js('new Audio(\"https://freesound.org/data/previews/80/80921_1022651-lq.ogg\").play()')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K6v7q8L6y8w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}